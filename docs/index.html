---

title: Welcome to 


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://raw.githubusercontent.com/matjesg/deepflash2/master/nbs/media/logo/logo_deepflash2_transp-02.png" alt="deepflash2"></p>
<p>Official repository of deepflash2 - a deep learning pipeline for segmentation of fluorescent labels in microscopy images.</p>
<p><img src="https://github.com/matjesg/deepflash2/workflows/CI/badge.svg" alt="CI"> 
<a href="https://pypi.org/project/deepflash2/#description"><img src="https://img.shields.io/pypi/v/deepflash2?color=blue&amp;label=pypi%20version" alt="PyPI"></a> 
<a href="https://pypistats.org/packages/deepflash2"><img src="https://img.shields.io/pypi/dm/deepflash2" alt="PyPI - Downloads"></a>
<a href="https://anaconda.org/matjesg/deepflash2"><img src="https://img.shields.io/conda/vn/matjesg/deepflash2?color=seagreen&amp;label=conda%20version" alt="Conda (channel only)"></a> 
<a href="https://github.com/matjesg/deepflash2"><img src="https://github.com/matjesg/deepflash2/workflows/Build%20deepflash2%20images/badge.svg" alt="Build fastai images"></a>
<a href="https://github.com/matjesg/deepflash2/"><img src="https://img.shields.io/github/stars/matjesg/deepflash2?style=social" alt="GitHub stars"></a>
<a href="https://github.com/matjesg/deepflash2/"><img src="https://img.shields.io/github/forks/matjesg/deepflash2?style=social" alt="GitHub forks"></a></p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Quick-Start-in-30-seconds">Quick Start in 30 seconds<a class="anchor-link" href="#Quick-Start-in-30-seconds"> </a></h2><p><a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/nbs/deepflash2.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab"></a></p>
<p><img src="https://raw.githubusercontent.com/matjesg/deepflash2/master/nbs/media/screen_captures/GUI_Train_start.gif" alt="deepflash2 training getting started"></p>
<p>Examplary training workflow.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-using-deepflash2?">Why using deepflash2?<a class="anchor-link" href="#Why-using-deepflash2?"> </a></h2><p><strong>The best of two worlds:</strong>
Combining state of the art deep learning with a barrier free environment for life science researchers.</p>
<ul>
<li>End-to-end process for life science researchers<ul>
<li>graphical user interface - no coding skills required</li>
<li>free usage on <em>Google Colab</em> at no costs</li>
<li>easy deployment on own hardware</li>
</ul>
</li>
<li>Rigorously evaluated deep learning models<ul>
<li>Model Library</li>
<li>easy integration new (<em>pytorch</em>) models</li>
</ul>
</li>
<li>Best practices model training<ul>
<li>leveraging the <em>fastai</em> library</li>
<li>mixed precision training</li>
<li>learning rate finder and fit one cycle policy </li>
<li>advanced augementation </li>
</ul>
</li>
<li>Reliable prediction on new data<ul>
<li>leveraging Bayesian Uncertainties</li>
</ul>
</li>
</ul>
<p><strong>Kaggle Gold Medal and Innovation Price Winner</strong></p>
<p><em>deepflash2</em> does not only work on fluorescent labels. The <em>deepflash2</em> API built the foundation for winning the <a href="https://hubmapconsortium.github.io/ccf/pages/kaggle.html">Innovation Award</a> a Kaggle Gold Medal in the <a href="https://www.kaggle.com/c/hubmap-kidney-segmentation">HuBMAP - Hacking the Kidney</a> challenge. 
Have a look at our <a href="https://www.kaggle.com/matjes/hubmap-deepflash2-judge-price">solution</a></p>
<p><img src="https://www.kaggle.com/static/images/medals/competitions/goldl@1x.png" alt="Gold Medal"></p>
<h2 id="Citing">Citing<a class="anchor-link" href="#Citing"> </a></h2><p>We're working on a peer reviewed publication. Until than, the preliminary citation is:</p>

<pre><code>@misc{griebel2021deepflash2,
  author = {Matthias Griebel},
  title = {DeepFLasH2 - a deep learning pipeline for segmentation of fluorescent labels in microscopy images},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/matjesg/deepflash2}}
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Workflow">Workflow<a class="anchor-link" href="#Workflow"> </a></h2><p>tbd</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installing">Installing<a class="anchor-link" href="#Installing"> </a></h2><p>You can use <strong>deepflash2</strong> by using <a href="colab.research.google.com">Google Colab</a>. You can run every page of the <a href="matjesg.github.io/deepflash2/">documentation</a> as an interactive notebook - click "Open in Colab" at the top of any page to open it.</p>
<ul>
<li>Be sure to change the Colab runtime to "GPU" to have it run fast!</li>
<li>Use Firefox or Google Chrome if you want to upload your images.</li>
</ul>
<p>You can install <strong>deepflash2</strong>  on your own machines with conda (highly recommended):</p>
<div class="highlight"><pre><span></span>conda install -c fastai -c pytorch -c matjesg deepflash2
</pre></div>
<p>To install with pip, use</p>
<div class="highlight"><pre><span></span>pip install deepflash2
</pre></div>
<p>If you install with pip, you should install PyTorch first by following the PyTorch <a href="https://pytorch.org/get-started/locally/">installation instructions</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-Docker">Using Docker<a class="anchor-link" href="#Using-Docker"> </a></h2><p>Docker images for <strong>deepflash2</strong> are built on top of <a href="https://hub.docker.com/r/pytorch/pytorch/">the latest pytorch image</a> and <a href="https://github.com/fastai/docker-containers">fastai</a> images. <strong>You must install <a href="https://github.com/NVIDIA/nvidia-docker">Nvidia-Docker</a> to enable gpu compatibility with these containers.</strong></p>
<ul>
<li>CPU only<blockquote><p><code>docker run -p 8888:8888 matjesg/deepflash</code></p>
</blockquote>
</li>
<li>With GPU support (<a href="https://github.com/NVIDIA/nvidia-docker">Nvidia-Docker</a> must be installed.)
has an editable install of fastai and fastcore.<blockquote><p><code>docker run --gpus all -p 8888:8888 matjesg/deepflash</code>
All docker containers are configured to start a jupyter server. <strong>deepflash2</strong> notebooks are available in the <code>deepflash2_notebooks</code> folder.</p>
</blockquote>
</li>
</ul>
<p>For more information on how to run docker see <a href="https://docs.docker.com/get-started/">docker orientation and setup</a> and <a href="https://github.com/fastai/docker-containers">fastai docker</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-Library">Model Library<a class="anchor-link" href="#Model-Library"> </a></h2><p>We provide a model library with pretrained model weights. Visit our <a href="https://matjesg.github.io/deepflash2/model_library.html">model library documentation</a> for information on the datasets of the pretrained models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Creating-segmentation-masks-with-Fiji/ImageJ">Creating segmentation masks with Fiji/ImageJ<a class="anchor-link" href="#Creating-segmentation-masks-with-Fiji/ImageJ"> </a></h2><p>If you don't have labelled training data available, you can use this <a href="https://github.com/matjesg/DeepFLaSH/raw/master/ImageJ/create_maps_howto.pdf">instruction manual</a> for creating segmentation maps.
The ImagJ-Macro is available <a href="https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/ImageJ/Macro_create_maps.ijm">here</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Acronym">Acronym<a class="anchor-link" href="#Acronym"> </a></h2><p>A Deep-learning pipeline for Fluorescent Label Segmentation that learns from Human experts</p>

</div>
</div>
</div>
</div>
 

