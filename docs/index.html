---

title: Welcome to


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://raw.githubusercontent.com/matjesg/deepflash2/master/nbs/media/logo/deepflash2_logo_medium.png" alt="deepflash2"></p>
<p>Official repository of deepflash2 - a deep-learning pipeline for segmentation of ambiguous microscopic images.</p>
<p><a href="https://pypi.org/project/deepflash2/#description"><img src="https://img.shields.io/pypi/v/deepflash2?color=blue&amp;label=pypi%20version" alt="PyPI"></a> 
<a href="https://pypistats.org/packages/deepflash2"><img src="https://img.shields.io/pypi/dm/deepflash2" alt="PyPI - Downloads"></a>
<a href="https://anaconda.org/matjesg/deepflash2"><img src="https://img.shields.io/conda/vn/matjesg/deepflash2?color=seagreen&amp;label=conda%20version" alt="Conda (channel only)"></a>
<a href="https://doi.org/10.5281/zenodo.7653312"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7653312.svg" alt="DOI"></a></p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>The best of two worlds:</strong>
Combining state-of-the-art deep learning with a barrier free environment for life science researchers.</p>
<blockquote><p>Read the <a href="https://arxiv.org/abs/2111.06693">paper</a>, watch the <a href="https://matjesg.github.io/deepflash2/tutorial.html">tutorials</a>, or read the <a href="https://matjesg.github.io/deepflash2/">docs</a>.</p>
<ul>
<li><strong>No coding skills required</strong> (graphical user interface)</li>
<li><strong>Ground truth estimation</strong> from the annotations of multiple experts for model training and validation</li>
<li><strong>Quality assurance and out-of-distribution detection</strong> for reliable prediction on new data </li>
<li><strong>Best-in-class performance</strong> for semantic and instance segmentation</li>
</ul>
</blockquote>
<p><img src="https://github.com/matjesg/deepflash2/blob/master/nbs/media/sample_images.png?raw=true" width="800px" style="max-width: 800pxpx"></p>
<p><img style="float: left;padding: 0px 10px 0px 0px;" src="https://www.kaggle.com/static/images/medals/competitions/goldl@1x.png"></p>
<p><strong>Kaggle Gold Medal and Innovation Price Winner:</strong> The <em>deepflash2</em> Python API built the foundation for winning the <a href="https://hubmapconsortium.github.io/ccf/pages/kaggle.html">Innovation Award</a> a Kaggle Gold Medal in the <a href="https://www.kaggle.com/c/hubmap-kidney-segmentation">HuBMAP - Hacking the Kidney</a> challenge. 
Have a look at our <a href="https://www.kaggle.com/matjes/hubmap-deepflash2-judge-price">solution</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Quick-Start-and-Demo">Quick Start and Demo<a class="anchor-link" href="#Quick-Start-and-Demo"> </a></h2><blockquote><p>Get started in less than a minute. Watch the <a href="https://matjesg.github.io/deepflash2/tutorial.html" target="_blank">tutorials</a> for help.</p>
<h4 id="Demo-on-Hugging-Face-Spaces">Demo on Hugging Face Spaces<a class="anchor-link" href="#Demo-on-Hugging-Face-Spaces"> </a></h4>
</blockquote>
<p>Go to the <a href="https://huggingface.co/spaces/matjesg/deepflash2">demo space</a> -- inference only (no training possible).</p>
<h4 id="Demo-usage-with-Google-Colab">Demo usage with Google Colab<a class="anchor-link" href="#Demo-usage-with-Google-Colab"> </a></h4><p>For a quick start, run <em>deepflash2</em> in Google Colaboratory (Google account required).</p>
<p><a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab"></a></p>
<video src="https://user-images.githubusercontent.com/13711052/139751414-acf737db-2d8a-4203-8a34-7a38e5326b5e.mov" controls width="100%"></video><p>The GUI provides a build-in use for our <a href="https://github.com/matjesg/deepflash2/releases/tag/sample_data">sample data</a>.</p>
<ol>
<li>Starting the GUI (in <a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb" target="_blank">Colab</a> or follow the installation instructions below)</li>
<li>Select the task (GT Estimation, Training, or Prediction) </li>
<li>Click the <code>Load Sample Data</code> button in the sidebar and continue to the next sidebar section.</li>
</ol>
<p>For futher instructions watch the <a href="https://matjesg.github.io/deepflash2/tutorial.html">tutorials</a>.</p>
<p>We provide an overview of the tasks below:</p>
<table>
<thead><tr>
<th></th>
<th>Ground Truth (GT) Estimation</th>
<th>Training</th>
<th>Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Task</td>
<td>STAPLE or Majority Voting</td>
<td>Ensemble training  and validation</td>
<td>Semantic and instance segmentation</td>
</tr>
<tr>
<td>Sample Data</td>
<td>5 masks from 5 experts each</td>
<td>5 image/mask pairs</td>
<td>5 images and 2 trained models</td>
</tr>
<tr>
<td>Expected Output</td>
<td>5 GT Segmentation Masks</td>
<td>5 models</td>
<td>5 predicted segmentation masks  (semantic and instance) and uncertainty maps</td>
</tr>
<tr>
<td>Estimated Time</td>
<td>~ 1 min</td>
<td>~ 150 min</td>
<td>~ 4 min</td>
</tr>
</tbody>
</table>
<p>Times are estimated for Google Colab (with free NVIDIA Tesla K80 GPU).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Paper-and-Experiments">Paper and Experiments<a class="anchor-link" href="#Paper-and-Experiments"> </a></h2><p>We provide a complete guide to reproduce our experiments using the <em>deepflash2 Python API</em> <a href="https://github.com/matjesg/deepflash2/tree/master/paper">here</a>. The data is currently available on <a href="https://drive.google.com/drive/folders/1r9AqP9qW9JThbMIvT0jhoA5mPxWEeIjs?usp=sharing">Google Drive</a> and <a href="https://doi.org/10.5281/zenodo.7653312">Zenodo</a>.</p>
<p>Our Nature Communications article is available <a href="https://www.nature.com/articles/s41467-023-36960-9">here</a>. Please cite</p>

<pre><code>@article{Griebel2023,
  doi = {10.1038/s41467-023-36960-9},
  url = {https://doi.org/10.1038/s41467-023-36960-9},
  year = {2023},
  month = mar,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {14},
  number = {1},
  author = {Matthias Griebel and Dennis Segebarth and Nikolai Stein and Nina Schukraft and Philip Tovote and Robert Blum and Christoph M. Flath},
  title = {Deep learning-enabled segmentation of ambiguous bioimages with deepflash2},
  journal = {Nature Communications}
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="System-requirements">System requirements<a class="anchor-link" href="#System-requirements"> </a></h2><blockquote><p>Works in the browser or on your local pc/server</p>
</blockquote>
<p><em>deepflash2</em> is designed to run on Windows, Linux, or Mac (x86-64) if <a href="https://pytorch.org/get-started/locally/">pytorch</a> is installable.
We generally recommend using Google Colab as it only requires a Google Account and a device with a web browser. 
To run <em>deepflash2</em> locally, we recommend using a system with a GPU (e.g., 2 CPUs, 8 GB RAM, NVIDIA GPU with 8GB VRAM or better).</p>
<p><em>deepflash2</em> requires Python&gt;3.6 and the software dependencies are defined in the <a href="https://github.com/matjesg/deepflash2/blob/master/settings.ini">settings.ini</a> file. Additionally, the ground truth estimation functionalities are based on simpleITK&gt;=2.0 and the instance segmentation capabilities are complemented using cellpose v0.6.6.dev13+g316927e.</p>
<p><em>deepflash2</em> is tested on Google Colab (Ubuntu 18.04.5 LTS) and locally (Ubuntu 20.04 LTS, Windows 10, MacOS 12.0.1).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installation-Guide">Installation Guide<a class="anchor-link" href="#Installation-Guide"> </a></h2><blockquote><p>Typical install time is about 1-5 minutes, depending on your internet connection</p>
</blockquote>
<p>The GUI of <em>deepflash2</em> runs as a web application inside a Jupyter Notebook, the de-facto standard of computational notebooks in the scientific community. The GUI is built on top of the <em>deepflash2</em> Python API, which can be used independently (read the <a href="https://matjesg.github.io/deepflash2/">docs</a>).</p>
<h3 id="Google-Colab">Google Colab<a class="anchor-link" href="#Google-Colab"> </a></h3><p><a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab"></a></p>
<p>Open <a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb" target="_blank">Colab</a> and excute the <code>Set up environment</code> cell or follow the <code>pip</code> instructions. Colab provides free access to graphics processing units (GPUs) for fast model training and prediction (Google account required).</p>
<h3 id="Other-systems">Other systems<a class="anchor-link" href="#Other-systems"> </a></h3><p>We recommend installation into a clean Python 3.7, 3.8, or 3.9 environment (e.g., using <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">conda</a>).</p>
<h4 id="mamba/conda"><a href="https://github.com/mamba-org/mamba">mamba</a>/<a href="https://docs.conda.io/en/latest/">conda</a><a class="anchor-link" href="#mamba/conda"> </a></h4><p>Installation with mamba (installaton <a href="https://github.com/mamba-org/mamba">instructions</a>) allows a fast and realiable installation process (you can replace <code>mamba</code> with <code>conda</code> and add the <code>--update-all</code> flag to do the installation with conda).</p>
<div class="highlight"><pre><span></span>mamba<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>fastchan<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-c<span class="w"> </span>matjesg<span class="w"> </span>deepflash2
</pre></div>
<h4 id="pip"><a href="https://pip.pypa.io/en/stable/">pip</a><a class="anchor-link" href="#pip"> </a></h4><p>If you want to use your GPU and install with pip, we recommend installing PyTorch first by following the <a href="https://pytorch.org/get-started/locally/">installation instructions</a>.</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>deepflash2
</pre></div>
<h4 id="Using-the-GUI">Using the GUI<a class="anchor-link" href="#Using-the-GUI"> </a></h4><p>If you want to use the GUI, make sure to download the GUI notebook, e.g., using <code>curl</code></p>
<div class="highlight"><pre><span></span>curl<span class="w"> </span>-o<span class="w"> </span>deepflash2_GUI.ipynb<span class="w"> </span>https://raw.githubusercontent.com/matjesg/deepflash2/master/deepflash2_GUI.ipynb
</pre></div>
<p>and start a Jupyter server.</p>
<div class="highlight"><pre><span></span>jupyter<span class="w"> </span>notebook
</pre></div>
<p>Then, open <code>deepflash2_GUI.ipynb</code> within Notebook environment.</p>
<h3 id="Docker">Docker<a class="anchor-link" href="#Docker"> </a></h3><p>Docker images for <strong>deepflash2</strong> are built on top of <a href="https://hub.docker.com/r/pytorch/pytorch/">the latest pytorch image</a>.</p>
<ul>
<li>CPU only<blockquote><p><code>docker run -p 8888:8888 matjes/deepflash2 ./run_jupyter.sh</code></p>
</blockquote>
</li>
<li>For training, we recommend to run docker with GPU support (You need to install <a href="https://github.com/NVIDIA/nvidia-docker">Nvidia-Docker</a> to enable gpu compatibility with these containers.)<blockquote><p><code>docker run --gpus all --shm-size=256m -p 8888:8888 matjes/deepflash2 ./run_jupyter.sh</code></p>
</blockquote>
</li>
</ul>
<p>All docker containers are configured to start a jupyter server. To add data, we recomment using <a href="https://docs.docker.com/storage/bind-mounts/">bind mounts</a> with <code>/workspace</code> as target. To start the GUI, open <code>deepflash2_GUI.ipynb</code> within Notebook environment.</p>
<p>For more information on how to run docker see <a href="https://docs.docker.com/get-started/">docker orientation and setup</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Creating-segmentation-masks-with-Fiji/ImageJ">Creating segmentation masks with Fiji/ImageJ<a class="anchor-link" href="#Creating-segmentation-masks-with-Fiji/ImageJ"> </a></h2><p>If you don't have labelled training data available, you can use this <a href="https://github.com/matjesg/DeepFLaSH/raw/master/ImageJ/create_maps_howto.pdf">instruction manual</a> for creating segmentation maps.
The ImagJ-Macro is available <a href="https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/ImageJ/Macro_create_maps.ijm">here</a>.</p>

</div>
</div>
</div>
</div>
 

