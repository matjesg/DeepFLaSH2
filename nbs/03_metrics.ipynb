{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp metrics\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> Definition of the metrics that can be used in training models. See [fastai2 docs](https://dev.fast.ai/metrics) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.metrics import Dice as fastai_Dice\n",
    "from fastai.torch_core import *\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastcore.foundation import patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice coefficient\n",
    "see [here](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou) for F1/Dice score discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dice(fastai_Dice):\n",
    "    \"Dice coefficient metric for binary target in segmentation\"\n",
    "    def accumulate(self, learn):\n",
    "        pred,targ = flatten_check(learn.pred.argmax(dim=self.axis), learn.yb[0])\n",
    "        pred, targ  = map(TensorBase, (pred, targ))\n",
    "        self.inter += (pred*targ).float().sum().item()\n",
    "        self.union += (pred+targ).float().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing: a fake learner and a metric that isn't an average\n",
    "@delegates()\n",
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through a fake cycle with various batch sizes and computes the value of met\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3): \n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(20,2,3,3)\n",
    "x2 = torch.randint(0, 2, (20, 3, 3))\n",
    "pred = x1.argmax(1)\n",
    "inter = (pred*x2).float().sum().item()\n",
    "union = (pred+x2).float().sum().item()\n",
    "x1 = TensorImage(x1)\n",
    "x2 = TensorMask(x2)\n",
    "test_eq(compute_val(Dice(), x1, x2), 2*inter/union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection-Over-Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Iou(Dice):\n",
    "    \"Implemetation of the IoU (jaccard coefficient) that is lighter in RAM\"\n",
    "    @property\n",
    "    def value(self): return self.inter/(self.union-self.inter) if self.union > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(compute_val(Iou(), x1, x2), inter/(union-inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch to show metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#from https://forums.fast.ai/t/plotting-metrics-after-learning/69937\n",
    "@patch\n",
    "@delegates(subplots)\n",
    "def plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    metrics = np.stack(self.values)\n",
    "    names = self.metric_names[1:-1]\n",
    "    n = len(names) - 1\n",
    "    if nrows is None and ncols is None:\n",
    "        nrows = int(math.sqrt(n))\n",
    "        ncols = int(np.ceil(n / nrows))\n",
    "    elif nrows is None: nrows = int(np.ceil(n / ncols))\n",
    "    elif ncols is None: ncols = int(np.ceil(n / nrows))\n",
    "    figsize = figsize or (ncols * 6, nrows * 4)\n",
    "    fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
    "    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n",
    "    for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)):\n",
    "        ax.plot(metrics[:, i], color='#1f77b4' if i == 0 else '#ff7f0e', label='valid' if i > 0 else 'train')\n",
    "        ax.set_title(name if i > 1 else 'losses')\n",
    "        ax.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_metrics.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted 07_tta.ipynb.\n",
      "Converted 08_gui.ipynb.\n",
      "Converted 09_gt.ipynb.\n",
      "Converted add_information.ipynb.\n",
      "Converted deepflash2.ipynb.\n",
      "Converted gt_estimation.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted model_library.ipynb.\n",
      "Converted predict.ipynb.\n",
      "Converted train.ipynb.\n",
      "Converted tutorial.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
